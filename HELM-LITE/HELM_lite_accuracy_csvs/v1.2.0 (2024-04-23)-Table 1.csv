Model,Mean win rate,NarrativeQA - F1,NaturalQuestions (open) - F1,NaturalQuestions (closed) - F1,OpenbookQA - EM,MMLU - EM,MATH - Equivalent (CoT),GSM8K - EM,LegalBench - EM,MedQA - EM,WMT 2014 - BLEU-4
GPT-4 (0613),0.958,0.768,0.79,0.457,0.96,0.735,0.802,0.932,0.713,0.815,0.211
Llama 3 (70B),0.903,0.798,0.743,0.475,0.934,0.695,0.663,0.805,0.733,0.777,0.225
Mixtral (8x22B),0.863,0.779,0.726,0.478,0.882,0.701,0.656,0.8,0.708,0.704,0.209
GPT-4 Turbo (1106 preview),0.825,0.727,0.763,0.435,0.95,0.699,0.857,0.668,0.626,0.817,0.205
Palmyra X V3 (72B),0.825,0.706,0.685,0.407,0.938,0.702,0.723,0.831,0.709,0.684,0.262
PaLM-2 (Unicorn),0.77,0.583,0.674,0.435,0.938,0.702,0.674,0.831,0.677,0.684,0.26
Palmyra X V2 (33B),0.758,0.752,0.752,0.428,0.878,0.621,0.58,0.735,0.644,0.598,0.239
Qwen1.5 (72B),0.753,0.601,0.758,0.417,0.93,0.647,0.683,0.799,0.694,0.67,0.201
Yi (34B),0.738,0.782,0.775,0.443,0.92,0.65,0.375,0.648,0.618,0.656,0.172
Anthropic Claude v1.3,0.703,0.723,0.699,0.409,0.908,0.631,0.54,0.784,0.629,0.618,0.219
Mixtral (8x7B 32K seqlen),0.695,0.767,0.699,0.427,0.868,0.649,0.494,0.622,0.63,0.652,0.19
Qwen1.5 (32B),0.69,0.589,0.777,0.353,0.932,0.628,0.733,0.773,0.636,0.656,0.193
PaLM-2 (Bison),0.673,0.718,0.813,0.39,0.878,0.608,0.421,0.61,0.645,0.547,0.241
Anthropic Claude 2.0,0.665,0.718,0.67,0.428,0.862,0.639,0.603,0.583,0.643,0.652,0.219
Llama 2 (70B),0.625,0.763,0.674,0.46,0.838,0.58,0.323,0.567,0.673,0.618,0.196
GPT-3.5 (text-davinci-003),0.593,0.731,0.77,0.413,0.828,0.555,0.449,0.615,0.622,0.531,0.191
Qwen1.5 (14B),0.59,0.711,0.772,0.3,0.862,0.626,0.686,0.693,0.593,0.515,0.178
Anthropic Claude 2.1,0.59,0.677,0.611,0.375,0.872,0.643,0.632,0.604,0.643,0.644,0.204
Anthropic Claude Instant 1.2,0.555,0.616,0.731,0.343,0.844,0.631,0.499,0.721,0.586,0.559,0.194
Llama 3 (8B),0.528,0.754,0.681,0.378,0.766,0.602,0.391,0.499,0.637,0.581,0.183
GPT-3.5 Turbo (0613),0.498,0.655,0.678,0.335,0.838,0.614,0.667,0.501,0.528,0.622,0.187
GPT-3.5 (text-davinci-002),0.475,0.719,0.71,0.394,0.796,0.568,0.428,0.479,0.58,0.525,0.174
Gemma (7B),0.473,0.752,0.665,0.336,0.808,0.571,0.5,0.559,0.581,0.513,0.187
LLaMA (65B),0.47,0.755,0.672,0.433,0.754,0.584,0.257,0.489,0.48,0.507,0.189
Cohere Command,0.43,0.749,0.777,0.391,0.774,0.525,0.236,0.452,0.578,0.445,0.088
Mistral v0.1 (7B),0.418,0.716,0.687,0.367,0.776,0.584,0.297,0.377,0.58,0.525,0.16
Mistral Medium (2312),0.395,0.449,0.468,0.29,0.83,0.618,0.565,0.706,0.452,0.61,0.169
Qwen1.5 (7B),0.388,0.448,0.749,0.27,0.806,0.569,0.561,0.6,0.523,0.479,0.153
Yi (6B),0.353,0.702,0.748,0.31,0.8,0.53,0.126,0.375,0.519,0.497,0.117
Llama 2 (13B),0.325,0.741,0.64,0.371,0.634,0.505,0.102,0.266,0.591,0.392,0.167
Jurassic-2 Jumbo (178B),0.313,0.728,0.65,0.385,0.688,0.483,0.103,0.239,0.533,0.431,0.114
Falcon (40B),0.303,0.671,0.676,0.392,0.662,0.507,0.128,0.267,0.442,0.419,0.162
Phi-2,0.263,0.703,0.68,0.155,0.798,0.518,0.255,0.581,0.334,0.41,0.038
Jurassic-2 Grande (17B),0.245,0.744,0.627,0.35,0.614,0.471,0.064,0.159,0.468,0.39,0.102
Llama 2 (7B),0.213,0.686,0.612,0.333,0.544,0.425,0.097,0.154,0.502,0.392,0.144
Luminous Supreme (70B),0.205,0.743,0.656,0.299,0.284,0.316,0.078,0.137,0.452,0.276,0.102
Cohere Command Light,0.153,0.629,0.686,0.195,0.398,0.386,0.098,0.149,0.397,0.312,0.023
Luminous Extended (30B),0.1,0.684,0.611,0.253,0.272,0.248,0.04,0.075,0.421,0.276,0.083
Falcon (7B),0.08,0.621,0.58,0.285,0.26,0.288,0.044,0.055,0.346,0.254,0.094
OLMo (7B),0.06,0.597,0.603,0.259,0.222,0.305,0.029,0.044,0.341,0.229,0.097
Luminous Base (13B),0.05,0.633,0.577,0.197,0.286,0.243,0.026,0.028,0.332,0.26,0.066